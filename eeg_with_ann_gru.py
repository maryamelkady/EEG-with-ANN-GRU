# -*- coding: utf-8 -*-
"""eeg-with-ann-gru.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F8dBm2UG1pL-YVlwX_7VdXwykEWATZl1
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.layers import Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import confusion_matrix, classification_report

data = pd.read_csv('/kaggle/input/emotions/emotions.csv')
data

sample = data.loc[0, 'fft_0_b':'fft_749_b']
plt.figure(figsize=(16, 10))
plt.plot(range(len(sample)), sample)
plt.title('Feautures fft_0_b through fft_749_b')
plt.show()

data['label'].value_counts()

label_mapping = {'NEGATIVE': 0, 'NEUTRAL': 1, 'POSITIVE': 2}

def preprocess_inputs(df):
  df = df.copy()

  df['label'] = df['label'].replace(label_mapping)

  y = df['label'].copy()
  X = df.drop('label', axis=1).copy()

  # Split data into train and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)

    # Further split the training set into training and validation sets
  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=123)

  return X_train, X_val, X_test, y_train, y_val, y_test

X_train, X_val, X_test, y_train, y_val, y_test = preprocess_inputs(data)
X_train

X_train.shape

# Define a custom layer that wraps your TensorFlow function
class MyCustomLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        # You can apply any TensorFlow function here
        return tf.nn.relu(inputs)  # Replace with your actual custom function

# Example input shape
inputs = tf.keras.Input(shape=(2548,))  # Shape: (None, 2548)

# Use a Lambda layer to expand dimensions
expand_dims = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=2))(inputs)  # Shape: (None, 1, 2548)

# GRU layer processing (Gated Recurrent Unit)
gru = tf.keras.layers.GRU(256, return_sequences=True)(expand_dims)  # Shape: (None, 1, 256)

# Apply the custom layer
processed = MyCustomLayer()(gru)  # Shape: (None, 1, 256)

# Flatten the output
flatten = tf.keras.layers.Flatten()(processed)  # Shape: (None, 256)

# Output layer
outputs = tf.keras.layers.Dense(3, activation='softmax')(flatten)  # Shape: (None, 3)

# Create the model
model = tf.keras.Model(inputs=inputs, outputs=outputs)

# Compile the model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',  # Use a loss function appropriate for classification
    metrics=['accuracy']
)

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Fit the model
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=16,
    callbacks=[early_stopping]
)

model_acc = model.evaluate(X_test, y_test, verbose=0)[1]
print("Test Accuracy: {:.3f}%".format(model_acc * 100))

import matplotlib.pyplot as plt

# Extract validation loss from the history object
val_loss = history.history['val_loss']

# Plot validation loss over epochs
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss', color='orange')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Validation Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

y_pred = np.array(list(map(lambda x: np.argmax(x), model.predict(X_test))))

cm = confusion_matrix(y_test, y_pred)
clr = classification_report(y_test, y_pred, target_names=label_mapping.keys())

plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')
plt.xticks(np.arange(3) + 0.5, label_mapping.keys())
plt.yticks(np.arange(3) + 0.5, label_mapping.keys())
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:\n----------------------\n", clr)